{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LibriSpeech dataset loading...\n",
      "Testing dataset download...\n",
      "Download attempt 1/3\n",
      "Download successful!\n",
      "Successfully downloaded dataset with 73 samples\n",
      "Loading Tiny LibriSpeech dataset...\n",
      "Dataset loaded with 73 samples\n",
      "\n",
      "Testing access to samples:\n",
      "\n",
      "Sample 0:\n",
      "Audio shape: torch.Size([80, 466])\n",
      "Text tokens shape: torch.Size([448])\n",
      "Transcript: A MAN SAID TO THE UNIVERSE SIR I EXIST\n",
      "\n",
      "Sample 1:\n",
      "Audio shape: torch.Size([80, 654])\n",
      "Text tokens shape: torch.Size([448])\n",
      "Transcript: SWEAT COVERED BRION'S BODY TRICKLING INTO THE TIGHT LOINCLOTH THAT WAS THE ONLY GARMENT HE WORE\n",
      "\n",
      "Sample 2:\n",
      "Audio shape: torch.Size([80, 1334])\n",
      "Text tokens shape: torch.Size([448])\n",
      "Transcript: THE CUT ON HIS CHEST STILL DRIPPING BLOOD THE ACHE OF HIS OVERSTRAINED EYES EVEN THE SOARING ARENA AROUND HIM WITH THE THOUSANDS OF SPECTATORS WERE TRIVIALITIES NOT WORTH THINKING ABOUT\n",
      "\n",
      "Testing batch loading:\n",
      "\n",
      "Batch 0:\n",
      "Audio batch shape: torch.Size([2, 80, 925])\n",
      "Text tokens batch shape: torch.Size([2, 448])\n",
      "\n",
      "Batch 1:\n",
      "Audio batch shape: torch.Size([2, 80, 1830])\n",
      "Text tokens batch shape: torch.Size([2, 448])\n",
      "\n",
      "Batch 2:\n",
      "Audio batch shape: torch.Size([2, 80, 2941])\n",
      "Text tokens batch shape: torch.Size([2, 448])\n",
      "\n",
      "Dataset test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size=51865):\n",
    "        self.pad_token = 0\n",
    "        self.eos_token = 1\n",
    "        self.bos_token = 2\n",
    "        self.char_to_idx = {chr(i): i+10 for i in range(ord('a'), ord('z')+1)}\n",
    "        self.char_to_idx.update({' ': 3, '.': 4, ',': 5, '!': 6, '?': 7, \"'\": 8, '\"': 9})\n",
    "        self.idx_to_char = {v: k for k, v in self.char_to_idx.items()}\n",
    "        self.language_codes = {'en': 0}\n",
    "        \n",
    "    def encode(self, text: str, max_length: int = 448) -> torch.Tensor:\n",
    "        tokens = [self.char_to_idx.get(c, self.char_to_idx[' ']) for c in text.lower()]\n",
    "        tokens = [self.bos_token] + tokens + [self.eos_token]\n",
    "        if len(tokens) < max_length:\n",
    "            tokens.extend([self.pad_token] * (max_length - len(tokens)))\n",
    "        else:\n",
    "            tokens = tokens[:max_length-1] + [self.eos_token]\n",
    "        return torch.tensor(tokens)\n",
    "    \n",
    "    def encode_language(self, language: str) -> torch.Tensor:\n",
    "        return torch.tensor([self.language_codes.get(language, 0)])\n",
    "\n",
    "class TinyLibriSpeechDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        print(\"Loading Tiny LibriSpeech dataset...\")\n",
    "        \n",
    "        # Use the tiny dummy dataset with 'clean' config\n",
    "        self.dataset = load_dataset(\n",
    "            \"patrickvonplaten/librispeech_asr_dummy\",\n",
    "            \"clean\",  # Specify the config\n",
    "            split=\"validation\"\n",
    "        )\n",
    "        \n",
    "        self.sample_rate = 16000\n",
    "        self.tokenizer = SimpleTokenizer()\n",
    "        \n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_mels=80,\n",
    "            n_fft=2048,\n",
    "            hop_length=160,\n",
    "            win_length=400\n",
    "        )\n",
    "        print(f\"Dataset loaded with {len(self.dataset)} samples\")\n",
    "    \n",
    "    def process_audio(self, audio: np.ndarray) -> torch.Tensor:\n",
    "        waveform = torch.from_numpy(audio).float()\n",
    "        if len(waveform.shape) > 1:\n",
    "            waveform = waveform.mean(dim=0)\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        mel_spec = torch.log(mel_spec + 1e-9)\n",
    "        return mel_spec\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        \n",
    "        # Process audio\n",
    "        audio_array = sample['audio']['array']\n",
    "        audio_features = self.process_audio(audio_array)\n",
    "        \n",
    "        # Get transcript\n",
    "        transcript = sample['text']\n",
    "        \n",
    "        # Convert to tokens\n",
    "        text_tokens = self.tokenizer.encode(transcript)\n",
    "        \n",
    "        return {\n",
    "            'audio_features': audio_features,\n",
    "            'text_tokens': text_tokens,\n",
    "            'transcript': transcript\n",
    "        }\n",
    "\n",
    "def try_download_dataset(max_retries=3, delay=5):\n",
    "    \"\"\"Try to download the dataset with retries\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Download attempt {attempt + 1}/{max_retries}\")\n",
    "            dataset = load_dataset(\n",
    "                \"patrickvonplaten/librispeech_asr_dummy\",\n",
    "                \"clean\",  # Specify the config\n",
    "                split=\"validation\"\n",
    "            )\n",
    "            print(\"Download successful!\")\n",
    "            return dataset\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Waiting {delay} seconds before retrying...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff\n",
    "    raise Exception(\"Failed to download dataset after all attempts\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable length sequences\"\"\"\n",
    "    \n",
    "    # Find max lengths in the batch\n",
    "    max_audio_len = max(b['audio_features'].shape[1] for b in batch)\n",
    "    \n",
    "    # Initialize tensors\n",
    "    batch_size = len(batch)\n",
    "    audio_features = torch.zeros(batch_size, 80, max_audio_len)\n",
    "    text_tokens = torch.stack([b['text_tokens'] for b in batch])\n",
    "    \n",
    "    # Fill in the tensors with padded data\n",
    "    for i, sample in enumerate(batch):\n",
    "        audio = sample['audio_features']\n",
    "        audio_len = audio.shape[1]\n",
    "        audio_features[i, :, :audio_len] = audio\n",
    "    \n",
    "    return {\n",
    "        'audio_features': audio_features,\n",
    "        'text_tokens': text_tokens,\n",
    "        'transcript': [b['transcript'] for b in batch]\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    print(\"Testing LibriSpeech dataset loading...\")\n",
    "    \n",
    "    try:\n",
    "        # First try just downloading\n",
    "        print(\"Testing dataset download...\")\n",
    "        test_dataset = try_download_dataset()\n",
    "        print(f\"Successfully downloaded dataset with {len(test_dataset)} samples\")\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = TinyLibriSpeechDataset()\n",
    "        \n",
    "        # Create dataloader\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=2,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        # Test by accessing a few samples\n",
    "        print(\"\\nTesting access to samples:\")\n",
    "        for idx in range(min(3, len(dataset))):\n",
    "            sample = dataset[idx]\n",
    "            print(f\"\\nSample {idx}:\")\n",
    "            print(f\"Audio shape: {sample['audio_features'].shape}\")\n",
    "            print(f\"Text tokens shape: {sample['text_tokens'].shape}\")\n",
    "            print(f\"Transcript: {sample['transcript']}\")\n",
    "        \n",
    "        print(\"\\nTesting batch loading:\")\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            print(f\"\\nBatch {batch_idx}:\")\n",
    "            print(f\"Audio batch shape: {batch['audio_features'].shape}\")\n",
    "            print(f\"Text tokens batch shape: {batch['text_tokens'].shape}\")\n",
    "            if batch_idx >= 2:\n",
    "                break\n",
    "                \n",
    "        print(\"\\nDataset test completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set longer timeout for downloads\n",
    "    import datasets.config as config\n",
    "    config.HF_DATASETS_HTTP_TIMEOUT = 1000  # 1000 seconds timeout\n",
    "    \n",
    "    # Run main\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Whisper training...\n",
      "Using device: cpu\n",
      "Loading Tiny LibriSpeech dataset...\n",
      "Dataset loaded with 73 samples\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error during execution: The size of tensor a (449) must match the size of tensor b (448) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (449) must match the size of tensor b (448) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 207\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m    205\u001b[0m config\u001b[38;5;241m.\u001b[39mHF_DATASETS_HTTP_TIMEOUT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m--> 207\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 179\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_whisper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m# Test inference\u001b[39;00m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTesting inference...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 114\u001b[0m, in \u001b[0;36mtrain_whisper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar):\n\u001b[0;32m--> 114\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Update progress bar\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 40\u001b[0m, in \u001b[0;36mWhisperTrainer.train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     37\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_loss(logits, text_tokens, task_type)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 195\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, audio_features, text_tokens, task_type)\u001b[0m\n\u001b[1;32m    189\u001b[0m task_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mtask_tokens[task_type]\n\u001b[1;32m    190\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[1;32m    191\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor([[task_token]])\u001b[38;5;241m.\u001b[39mexpand(text_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(text_tokens\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    192\u001b[0m     text_tokens\n\u001b[1;32m    193\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 195\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 163\u001b[0m, in \u001b[0;36mWhisperDecoder.forward\u001b[0;34m(self, x, encoder_out, mask)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, encoder_out: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    161\u001b[0m             mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    162\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding(x)\n\u001b[0;32m--> 163\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_embedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m    166\u001b[0m         x \u001b[38;5;241m=\u001b[39m block(x, encoder_out, mask)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (449) must match the size of tensor b (448) at non-singleton dimension 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/margokim/fsl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size=51865):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_token = 0\n",
    "        self.eos_token = 1\n",
    "        self.bos_token = 2\n",
    "        \n",
    "        # Task tokens start from end of vocabulary\n",
    "        self.task_tokens = {\n",
    "            'transcribe': vocab_size - 5,\n",
    "            'translate': vocab_size - 4,\n",
    "            'language_id': vocab_size - 3,\n",
    "            'no_timestamps': vocab_size - 2,\n",
    "            'no_speech': vocab_size - 1\n",
    "        }\n",
    "        \n",
    "        # Simple character-level tokenization\n",
    "        self.char_to_idx = {chr(i): i+10 for i in range(ord('a'), ord('z')+1)}\n",
    "        self.char_to_idx.update({\n",
    "            ' ': 3, '.': 4, ',': 5, '!': 6, '?': 7, \"'\": 8, '\"': 9\n",
    "        })\n",
    "        self.idx_to_char = {v: k for k, v in self.char_to_idx.items()}\n",
    "        self.language_codes = {'en': 0}\n",
    "        \n",
    "    def encode(self, text: str, max_length: int = 448) -> torch.Tensor:\n",
    "        tokens = [self.char_to_idx.get(c, self.char_to_idx[' ']) for c in text.lower()]\n",
    "        tokens = [self.bos_token] + tokens + [self.eos_token]\n",
    "        \n",
    "        if len(tokens) < max_length:\n",
    "            tokens.extend([self.pad_token] * (max_length - len(tokens)))\n",
    "        else:\n",
    "            tokens = tokens[:max_length-1] + [self.eos_token]\n",
    "        return torch.tensor(tokens)\n",
    "    \n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        text = []\n",
    "        for token in tokens:\n",
    "            if token == self.eos_token:\n",
    "                break\n",
    "            if token.item() in self.idx_to_char:\n",
    "                text.append(self.idx_to_char[token.item()])\n",
    "        return ''.join(text)\n",
    "    \n",
    "    def encode_language(self, language: str) -> torch.Tensor:\n",
    "        return torch.tensor([self.language_codes.get(language, 0)])\n",
    "\n",
    "class TinyLibriSpeechDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        print(\"Loading Tiny LibriSpeech dataset...\")\n",
    "        \n",
    "        # Use the tiny dummy dataset with 'clean' config\n",
    "        self.dataset = load_dataset(\n",
    "            \"patrickvonplaten/librispeech_asr_dummy\",\n",
    "            \"clean\",\n",
    "            split=\"validation\"\n",
    "        )\n",
    "        \n",
    "        self.sample_rate = 16000\n",
    "        self.tokenizer = SimpleTokenizer()\n",
    "        \n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_mels=80,\n",
    "            n_fft=2048,\n",
    "            hop_length=160,\n",
    "            win_length=400\n",
    "        )\n",
    "        print(f\"Dataset loaded with {len(self.dataset)} samples\")\n",
    "    \n",
    "    def process_audio(self, audio: np.ndarray) -> torch.Tensor:\n",
    "        waveform = torch.from_numpy(audio).float()\n",
    "        if len(waveform.shape) > 1:\n",
    "            waveform = waveform.mean(dim=0)\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        mel_spec = torch.log(mel_spec + 1e-9)\n",
    "        return mel_spec\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        \n",
    "        # Process audio\n",
    "        audio_array = sample['audio']['array']\n",
    "        audio_features = self.process_audio(audio_array)\n",
    "        \n",
    "        # Get transcript\n",
    "        transcript = sample['text']\n",
    "        \n",
    "        # Convert to tokens\n",
    "        text_tokens = self.tokenizer.encode(transcript)\n",
    "        \n",
    "        return {\n",
    "            'audio_features': audio_features,\n",
    "            'text_tokens': text_tokens,\n",
    "            'transcript': transcript\n",
    "        }\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, kv: torch.Tensor = None,\n",
    "                mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(kv)\n",
    "        v = self.value(kv)\n",
    "\n",
    "        head_dim = q.size(-1) // self.n_head\n",
    "        q = q.view(*q.shape[:-1], self.n_head, head_dim).transpose(-3, -2)\n",
    "        k = k.view(*k.shape[:-1], self.n_head, head_dim).transpose(-3, -2)\n",
    "        v = v.view(*v.shape[:-1], self.n_head, head_dim).transpose(-3, -2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(-3, -2).contiguous()\n",
    "        out = out.view(*out.shape[:-2], -1)\n",
    "        \n",
    "        return self.out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n",
    "        super().__init__()\n",
    "        self.cross_attention = cross_attention\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        if cross_attention:\n",
    "            self.cross_attn = MultiHeadAttention(n_state, n_head)\n",
    "            self.ln_cross = nn.LayerNorm(n_state)\n",
    "        self.ln1 = nn.LayerNorm(n_state)\n",
    "        self.ln2 = nn.LayerNorm(n_state)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_state, 4 * n_state),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_state, n_state)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_out: torch.Tensor = None,\n",
    "                mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        x = x + self.attn(self.ln1(x), mask=mask)\n",
    "        if self.cross_attention and encoder_out is not None:\n",
    "            x = x + self.cross_attn(self.ln_cross(x), encoder_out)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Whisper training...\n",
      "Saving checkpoints to: /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints\n",
      "Using device: cpu\n",
      "Loading Tiny LibriSpeech dataset...\n",
      "Dataset loaded with 73 samples\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/19 [00:04<?, ?it/s, loss=1.0839]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving best model (loss: 1.0839) to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_0_batch_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  53%|█████▎    | 10/19 [00:41<00:28,  3.18s/it, loss=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving best model (loss: 0.1406) to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_0_batch_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 19/19 [00:59<00:00,  3.12s/it, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 completed, Average Loss: 0.0824\n",
      "Saved epoch checkpoint to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_epoch_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/19 [00:03<?, ?it/s, loss=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving best model (loss: 0.0000) to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_1_batch_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 19/19 [00:56<00:00,  2.99s/it, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 completed, Average Loss: 0.0000\n",
      "Saved epoch checkpoint to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 19/19 [00:43<00:00,  2.29s/it, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 completed, Average Loss: 0.0000\n",
      "Saved epoch checkpoint to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 19/19 [00:47<00:00,  2.49s/it, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 completed, Average Loss: 0.0000\n",
      "Saved epoch checkpoint to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 19/19 [00:42<00:00,  2.22s/it, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 completed, Average Loss: 0.0000\n",
      "Saved epoch checkpoint to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 19/19 [00:43<00:00,  2.28s/it, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 completed, Average Loss: 0.0000\n",
      "Saved epoch checkpoint to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_epoch_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 19/19 [00:50<00:00,  2.67s/it, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 completed, Average Loss: 0.0000\n",
      "Saved epoch checkpoint to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_epoch_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 19/19 [01:00<00:00,  3.19s/it, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 completed, Average Loss: 0.0000\n",
      "Saved epoch checkpoint to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_epoch_7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 19/19 [00:45<00:00,  2.40s/it, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 completed, Average Loss: 0.0000\n",
      "Saved epoch checkpoint to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_epoch_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 19/19 [01:04<00:00,  3.40s/it, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 completed, Average Loss: 0.0000\n",
      "Saved epoch checkpoint to /var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_epoch_9.pt\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "class WhisperEncoder(nn.Module):\n",
    "    def __init__(self, n_mels: int = 80, n_ctx: int = 1500, \n",
    "                 n_state: int = 512, n_head: int = 8, n_layer: int = 6):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_mels, n_state, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(n_state, n_state, 3, stride=2, padding=1)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(n_state, n_head) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(n_state)\n",
    "        \n",
    "        # Initialize weights\n",
    "        torch.nn.init.normal_(self.positional_embedding, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x + self.positional_embedding[:x.shape[1], :]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "class WhisperDecoder(nn.Module):\n",
    "    def __init__(self, n_vocab: int = 51865, n_ctx: int = 448,\n",
    "                 n_state: int = 512, n_head: int = 8, n_layer: int = 6):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(n_state, n_head, cross_attention=True)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(n_state)\n",
    "        self.fc = nn.Linear(n_state, n_vocab, bias=False)\n",
    "        \n",
    "        # Tie weights with embedding\n",
    "        self.fc.weight = self.token_embedding.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        torch.nn.init.normal_(self.positional_embedding, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_out: torch.Tensor,\n",
    "                mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # Ensure input sequence length doesn't exceed positional embedding size\n",
    "        x = self.token_embedding(x[:, :self.positional_embedding.size(0)])\n",
    "        x = x + self.positional_embedding[:x.shape[1], :]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, encoder_out, mask)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class WhisperModel(nn.Module):\n",
    "    def __init__(self, n_mels: int = 80, n_vocab: int = 51865,\n",
    "                 n_state: int = 512, n_head: int = 8, n_layer: int = 6):\n",
    "        super().__init__()\n",
    "        self.encoder = WhisperEncoder(n_mels, n_state=n_state,\n",
    "                                    n_head=n_head, n_layer=n_layer)\n",
    "        self.decoder = WhisperDecoder(n_vocab, n_state=n_state,\n",
    "                                    n_head=n_head, n_layer=n_layer)\n",
    "        \n",
    "        # Task tokens are handled by the tokenizer\n",
    "        self.tokenizer = SimpleTokenizer(n_vocab)\n",
    "        \n",
    "        # Initialize task embeddings\n",
    "        self.task_tokens = nn.Parameter(torch.randn(len(self.tokenizer.task_tokens), n_state))\n",
    "\n",
    "    def forward(self, audio_features: torch.Tensor, text_tokens: torch.Tensor,\n",
    "                task_type: str = 'transcribe') -> torch.Tensor:\n",
    "        encoder_out = self.encoder(audio_features)\n",
    "        \n",
    "        # Get task token embedding\n",
    "        task_idx = self.tokenizer.task_tokens[task_type]\n",
    "        task_embedding = self.task_tokens[task_idx - (self.tokenizer.vocab_size - len(self.tokenizer.task_tokens))]\n",
    "        \n",
    "        # Create decoder input sequence with task token\n",
    "        batch_size = text_tokens.size(0)\n",
    "        task_emb = task_embedding.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, -1)\n",
    "        \n",
    "        # Get text embeddings\n",
    "        text_emb = self.decoder.token_embedding(text_tokens)\n",
    "        \n",
    "        # Combine task and text embeddings\n",
    "        decoder_input = torch.cat([task_emb, text_emb], dim=1)\n",
    "        \n",
    "        # Add positional embeddings and decode\n",
    "        decoder_input = decoder_input + self.decoder.positional_embedding[:decoder_input.size(1), :]\n",
    "        \n",
    "        # Run through decoder blocks\n",
    "        for block in self.decoder.blocks:\n",
    "            decoder_input = block(decoder_input, encoder_out)\n",
    "            \n",
    "        decoder_input = self.decoder.ln(decoder_input)\n",
    "        logits = self.decoder.fc(decoder_input)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class WhisperTrainer:\n",
    "    def __init__(self, model: WhisperModel, learning_rate: float = 1e-4):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=100000, eta_min=learning_rate/10\n",
    "        )\n",
    "    \n",
    "    def prepare_targets(self, text_tokens: torch.Tensor, task_type: str) -> torch.Tensor:\n",
    "        \"\"\"Prepare target sequence including task token\"\"\"\n",
    "        batch_size = text_tokens.size(0)\n",
    "        device = text_tokens.device\n",
    "        \n",
    "        # Get task token\n",
    "        task_token = self.model.tokenizer.task_tokens[task_type]\n",
    "        \n",
    "        # Create targets with task token at the beginning\n",
    "        targets = torch.cat([\n",
    "            torch.full((batch_size, 1), task_token, device=device),\n",
    "            text_tokens\n",
    "        ], dim=1)\n",
    "        \n",
    "        return targets\n",
    "        \n",
    "    def train_step(self, batch: dict) -> float:\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Move batch to same device as model\n",
    "        device = next(self.model.parameters()).device\n",
    "        audio_features = batch['audio_features'].to(device)\n",
    "        text_tokens = batch['text_tokens'].to(device)\n",
    "        \n",
    "        # Prepare targets with task token\n",
    "        targets = self.prepare_targets(text_tokens, 'transcribe')\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = self.model(audio_features, text_tokens, 'transcribe')\n",
    "        \n",
    "        # Calculate loss including task token prediction\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            targets.view(-1),\n",
    "            ignore_index=0  # Only ignore padding tokens\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable length sequences\"\"\"\n",
    "    \n",
    "    # Find max lengths in the batch\n",
    "    max_audio_len = max(b['audio_features'].shape[1] for b in batch)\n",
    "    max_text_len = min(447, max(b['text_tokens'].shape[0] for b in batch))  # 447 to leave room for task token\n",
    "    \n",
    "    # Initialize tensors\n",
    "    batch_size = len(batch)\n",
    "    audio_features = torch.zeros(batch_size, 80, max_audio_len)\n",
    "    text_tokens = torch.zeros(batch_size, max_text_len).long()\n",
    "    \n",
    "    # Fill in the tensors with padded data\n",
    "    for i, sample in enumerate(batch):\n",
    "        # Audio features\n",
    "        audio = sample['audio_features']\n",
    "        audio_len = audio.shape[1]\n",
    "        audio_features[i, :, :audio_len] = audio\n",
    "        \n",
    "        # Text tokens (truncate if needed)\n",
    "        text = sample['text_tokens'][:max_text_len]\n",
    "        text_len = text.shape[0]\n",
    "        text_tokens[i, :text_len] = text\n",
    "    \n",
    "    return {\n",
    "        'audio_features': audio_features,\n",
    "        'text_tokens': text_tokens,\n",
    "        'transcript': [b['transcript'] for b in batch]\n",
    "    }\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "def create_checkpoint_dir():\n",
    "    \"\"\"Create checkpoint directory in temporary directory\"\"\"\n",
    "    # Use system's temp directory\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    checkpoint_dir = Path(temp_dir) / \"whisper_checkpoints\"\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    return checkpoint_dir\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing Whisper training...\")\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = create_checkpoint_dir()\n",
    "    print(f\"Saving checkpoints to: {checkpoint_dir}\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = WhisperModel(\n",
    "        n_mels=80,\n",
    "        n_vocab=51865,\n",
    "        n_state=512,\n",
    "        n_head=8,\n",
    "        n_layer=6\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TinyLibriSpeechDataset()\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = WhisperTrainer(model)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            try:\n",
    "                loss = trainer.train_step(batch)\n",
    "                epoch_loss += loss\n",
    "                \n",
    "                progress_bar.set_postfix({'loss': f'{loss:.4f}'})\n",
    "                \n",
    "                # Save checkpoint if loss improved\n",
    "                if batch_idx % 10 == 0:\n",
    "                    avg_loss = epoch_loss / (batch_idx + 1)\n",
    "                    if avg_loss < best_loss:\n",
    "                        best_loss = avg_loss\n",
    "                        checkpoint_path = checkpoint_dir / f'whisper_best_epoch_{epoch}_batch_{batch_idx}.pt'\n",
    "                        print(f\"\\nSaving best model (loss: {best_loss:.4f}) to {checkpoint_path}\")\n",
    "                        try:\n",
    "                            torch.save({\n",
    "                                'epoch': epoch,\n",
    "                                'batch_idx': batch_idx,\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                                'loss': best_loss,\n",
    "                            }, checkpoint_path)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error saving checkpoint: {str(e)}\")\n",
    "                            continue\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"Error in training step: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        print(f'\\nEpoch {epoch+1} completed, Average Loss: {avg_epoch_loss:.4f}')\n",
    "        \n",
    "        # Save epoch checkpoint\n",
    "        try:\n",
    "            checkpoint_path = checkpoint_dir / f'whisper_epoch_{epoch}.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                'loss': avg_epoch_loss,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Saved epoch checkpoint to {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving epoch checkpoint: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Set longer timeout for downloads\n",
    "        import datasets.config as config\n",
    "        config.HF_DATASETS_HTTP_TIMEOUT = 1000\n",
    "        \n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path : /private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_4_batch_0.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic transcription tests...\n",
      "Starting Whisper model testing...\n",
      "Loading test dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Run basic tests\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning basic transcription tests...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Run feature tests\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning feature tests...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 102\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Load test dataset (VoxPopuli English samples)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading test dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/voxpopuli\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest[:10]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load only 10 samples\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Initialize tester with your checkpoint\u001b[39;00m\n\u001b[1;32m    105\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_4_batch_0.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/load.py:2154\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2154\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2164\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2165\u001b[0m )\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 924\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/builder.py:1648\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1648\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/builder.py:978\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[1;32m    977\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m--> 978\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msplit_generators_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/facebook--voxpopuli/b5ff837284f0778eefe0f642734e142d8c3f574eba8c9c8a4b13602297f73604/voxpopuli.py:143\u001b[0m, in \u001b[0;36mVoxpopuli._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# dl_manager.download_config.num_proc = len(urls)\u001b[39;00m\n\u001b[1;32m    142\u001b[0m meta_paths \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownload_and_extract(meta_urls)\n\u001b[0;32m--> 143\u001b[0m audio_paths \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_urls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m local_extracted_audio_paths \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    146\u001b[0m     dl_manager\u001b[38;5;241m.\u001b[39mextract(audio_paths) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mis_streaming \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     {\n\u001b[1;32m    148\u001b[0m         split: {lang: [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(audio_paths[split][lang]) \u001b[38;5;28;01mfor\u001b[39;00m lang \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlanguages} \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits\n\u001b[1;32m    149\u001b[0m     }\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_accented\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/download/download_manager.py:159\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    157\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[0;32m--> 159\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading data files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/utils/py_utils.py:511\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[0;32m--> 511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/utils/py_utils.py:512\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m    511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 512\u001b[0m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    514\u001b[0m ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/utils/py_utils.py:399\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    396\u001b[0m         k: _single_map_nested((function, v, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pbar\n\u001b[1;32m    397\u001b[0m     }\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mapped\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/utils/py_utils.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    396\u001b[0m         k: _single_map_nested((function, v, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pbar\n\u001b[1;32m    397\u001b[0m     }\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [\u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m pbar]\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mapped\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/utils/py_utils.py:395\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(pbar_iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, position\u001b[38;5;241m=\u001b[39mrank, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj\u001b[39m\u001b[38;5;124m\"\u001b[39m, desc\u001b[38;5;241m=\u001b[39mpbar_desc) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 395\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m [_single_map_nested((function, v, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m pbar]\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/utils/py_utils.py:396\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(pbar_iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, position\u001b[38;5;241m=\u001b[39mrank, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj\u001b[39m\u001b[38;5;124m\"\u001b[39m, desc\u001b[38;5;241m=\u001b[39mpbar_desc) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m--> 396\u001b[0m             k: \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pbar\n\u001b[1;32m    397\u001b[0m         }\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m [_single_map_nested((function, v, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m pbar]\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/utils/py_utils.py:380\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m     batched\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    379\u001b[0m ):\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mmapped_item\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmapped_item\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/utils/py_utils.py:380\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m     batched\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    379\u001b[0m ):\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/download/download_manager.py:219\u001b[0m, in \u001b[0;36mDownloadManager._download_batched\u001b[0;34m(self, url_or_filenames, download_config)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[1;32m    207\u001b[0m         download_func,\n\u001b[1;32m    208\u001b[0m         url_or_filenames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl_or_filenames\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/download/download_manager.py:220\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[1;32m    207\u001b[0m         download_func,\n\u001b[1;32m    208\u001b[0m         url_or_filenames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 220\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[1;32m    222\u001b[0m     ]\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/download/download_manager.py:229\u001b[0m, in \u001b[0;36mDownloadManager._download_single\u001b[0;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m url_or_path_join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path, url_or_filename)\n\u001b[0;32m--> 229\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m out \u001b[38;5;241m=\u001b[39m tracked_str(out)\n\u001b[1;32m    231\u001b[0m out\u001b[38;5;241m.\u001b[39mset_origin(url_or_filename)\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/datasets/utils/file_utils.py:188\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m resolved_path \u001b[38;5;241m=\u001b[39m huggingface_hub\u001b[38;5;241m.\u001b[39mHfFileSystem(\n\u001b[1;32m    179\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mHF_ENDPOINT, token\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mtoken\n\u001b[1;32m    180\u001b[0m )\u001b[38;5;241m.\u001b[39mresolve_path(url_or_filename)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mhuggingface_hub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHfApi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHF_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_datasets_user_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 188\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    197\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mRepositoryNotFoundError,\n\u001b[1;32m    198\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mEntryNotFoundError,\n\u001b[1;32m    199\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mRevisionNotFoundError,\n\u001b[1;32m    200\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGatedRepoError,\n\u001b[1;32m    201\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/huggingface_hub/hf_api.py:5548\u001b[0m, in \u001b[0;36mHfApi.hf_hub_download\u001b[0;34m(self, repo_id, filename, subfolder, repo_type, revision, cache_dir, local_dir, force_download, proxies, etag_timeout, token, local_files_only, resume_download, legacy_cache_layout, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   5544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5545\u001b[0m     \u001b[38;5;66;03m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[1;32m   5546\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m-> 5548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5551\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5560\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5564\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegacy_cache_layout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegacy_cache_layout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5570\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1230\u001b[0m     )\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/huggingface_hub/file_download.py:1381\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1379\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1381\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1392\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/huggingface_hub/file_download.py:1915\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1913\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1915\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1924\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1925\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    539\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    543\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/urllib3/response.py:934\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 934\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    937\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/urllib3/response.py:877\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 877\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/urllib3/response.py:812\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    809\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 812\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/urllib3/response.py:797\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def test_model_features():\n",
    "    \"\"\"Test different features of the model\"\"\"\n",
    "    checkpoint_path = \"/private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_4_batch_0.pt\"\n",
    "    tester = WhisperTester(checkpoint_path)\n",
    "    \n",
    "    print(\"Testing model features...\")\n",
    "    \n",
    "    # Load a single test sample\n",
    "    dataset = load_dataset(\"facebook/voxpopuli\", \"en\", split=\"test[:1]\")\n",
    "    sample = dataset[0]\n",
    "    audio_features = tester.process_audio(sample['audio']['array'])\n",
    "    \n",
    "    print(\"\\n1. Basic Transcription Test\")\n",
    "    transcription = tester.transcribe(audio_features)\n",
    "    print(f\"Original: {sample['text']}\")\n",
    "    print(f\"Predicted: {transcription}\")\n",
    "    \n",
    "    print(\"\\n2. Testing Attention Patterns\")\n",
    "    with torch.no_grad():\n",
    "        # Get encoder-decoder attention weights\n",
    "        audio_features = audio_features.unsqueeze(0).to(tester.device)\n",
    "        encoder_out = tester.model.encoder(audio_features)\n",
    "        \n",
    "        # Get initial decoder input\n",
    "        decoder_input = torch.tensor([[\n",
    "            tester.model.tokenizer.task_tokens['transcribe'],\n",
    "            tester.model.tokenizer.bos_token\n",
    "        ]]).to(tester.device)\n",
    "        \n",
    "        # Get attention patterns from first decoder block\n",
    "        first_block = tester.model.decoder.blocks[0]\n",
    "        _ = first_block(\n",
    "            tester.model.decoder.token_embedding(decoder_input),\n",
    "            encoder_out\n",
    "        )\n",
    "        \n",
    "        print(\"Encoder output shape:\", encoder_out.shape)\n",
    "        print(\"Decoder input shape:\", decoder_input.shape)\n",
    "    \n",
    "    print(\"\\n3. Testing Task Token Impact\")\n",
    "    # Test transcription with different task tokens\n",
    "    tasks = ['transcribe', 'translate', 'language_id']\n",
    "    for task in tasks:\n",
    "        print(f\"\\nTesting {task} task:\")\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # Prepare input with different task token\n",
    "                decoder_input = torch.tensor([[\n",
    "                    tester.model.tokenizer.task_tokens[task],\n",
    "                    tester.model.tokenizer.bos_token\n",
    "                ]]).to(tester.device)\n",
    "                \n",
    "                # Get initial output\n",
    "                logits = tester.model.decoder(decoder_input, encoder_out)\n",
    "                print(f\"Output logits shape: {logits.shape}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error testing {task}: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nFeature testing completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run basic tests\n",
    "    print(\"Running basic transcription tests...\")\n",
    "    test_model()\n",
    "    \n",
    "    # Run feature tests\n",
    "    print(\"\\nRunning feature tests...\")\n",
    "    test_model_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting quick Whisper model test...\n",
      "Loading minimal test dataset...\n",
      "Using device: cpu\n",
      "Loading model from /private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_4_batch_0.pt\n",
      "Loaded model from epoch 4, loss: 0.0000\n",
      "\n",
      "Testing transcription on samples:\n",
      "\n",
      "Sample 1/5:\n",
      "Original: A MAN SAID TO THE UNIVERSE SIR I EXIST\n",
      "Predicted: \n",
      "Word Error Rate: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 2/5:\n",
      "Original: SWEAT COVERED BRION'S BODY TRICKLING INTO THE TIGHT LOINCLOTH THAT WAS THE ONLY GARMENT HE WORE\n",
      "Predicted: \n",
      "Word Error Rate: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 3/5:\n",
      "Original: THE CUT ON HIS CHEST STILL DRIPPING BLOOD THE ACHE OF HIS OVERSTRAINED EYES EVEN THE SOARING ARENA AROUND HIM WITH THE THOUSANDS OF SPECTATORS WERE TRIVIALITIES NOT WORTH THINKING ABOUT\n",
      "Predicted: \n",
      "Word Error Rate: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 4/5:\n",
      "Original: HIS INSTANT OF PANIC WAS FOLLOWED BY A SMALL SHARP BLOW HIGH ON HIS CHEST\n",
      "Predicted: \n",
      "Word Error Rate: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 5/5:\n",
      "Original: ONE MINUTE A VOICE SAID AND THE TIME BUZZER SOUNDED\n",
      "Predicted: \n",
      "Word Error Rate: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Quick test completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class WhisperTester:\n",
    "    def __init__(self, checkpoint_path, device=None):\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.model = self.load_model(checkpoint_path)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.sample_rate = 16000\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_mels=80,\n",
    "            n_fft=2048,\n",
    "            hop_length=160,\n",
    "            win_length=400\n",
    "        )\n",
    "    \n",
    "    def load_model(self, checkpoint_path):\n",
    "        print(f\"Loading model from {checkpoint_path}\")\n",
    "        model = WhisperModel(\n",
    "            n_mels=80,\n",
    "            n_vocab=51865,\n",
    "            n_state=512,\n",
    "            n_head=8,\n",
    "            n_layer=6\n",
    "        ).to(self.device)\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=True)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded model from epoch {checkpoint['epoch']}, loss: {checkpoint['loss']:.4f}\")\n",
    "        return model\n",
    "    \n",
    "    def process_audio(self, audio: np.ndarray) -> torch.Tensor:\n",
    "        waveform = torch.from_numpy(audio).float()\n",
    "        if len(waveform.shape) > 1:\n",
    "            waveform = waveform.mean(dim=0)\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        mel_spec = torch.log(mel_spec + 1e-9)\n",
    "        return mel_spec\n",
    "    \n",
    "    def transcribe(self, audio_features: torch.Tensor) -> str:\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Ensure audio features have batch dimension\n",
    "            if len(audio_features.shape) == 2:\n",
    "                audio_features = audio_features.unsqueeze(0)\n",
    "            audio_features = audio_features.to(self.device)\n",
    "            \n",
    "            # Get task token embedding\n",
    "            task_idx = self.model.tokenizer.task_tokens['transcribe']\n",
    "            task_embedding = self.model.task_tokens[task_idx - (self.model.tokenizer.vocab_size - \n",
    "                                                              len(self.model.tokenizer.task_tokens))]\n",
    "            \n",
    "            # Initialize decoder input with task token\n",
    "            batch_size = audio_features.size(0)\n",
    "            task_emb = task_embedding.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, -1)\n",
    "            \n",
    "            # Encode audio\n",
    "            encoder_out = self.model.encoder(audio_features)\n",
    "            \n",
    "            # Initialize decoder sequence\n",
    "            decoder_input = self.model.decoder.token_embedding(\n",
    "                torch.tensor([[self.model.tokenizer.bos_token]], device=self.device)\n",
    "            )\n",
    "            decoder_input = torch.cat([task_emb, decoder_input], dim=1)\n",
    "            \n",
    "            # Generate sequence\n",
    "            generated_tokens = []\n",
    "            max_length = 100\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                # Add positional embeddings\n",
    "                pos_emb = self.model.decoder.positional_embedding[:decoder_input.size(1), :].unsqueeze(0)\n",
    "                decoder_input = decoder_input + pos_emb\n",
    "                \n",
    "                # Run through decoder blocks\n",
    "                x = decoder_input\n",
    "                for block in self.model.decoder.blocks:\n",
    "                    x = block(x, encoder_out)\n",
    "                \n",
    "                # Get next token\n",
    "                x = self.model.decoder.ln(x)\n",
    "                logits = self.model.decoder.fc(x)\n",
    "                next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "                \n",
    "                if next_token.item() == self.model.tokenizer.eos_token:\n",
    "                    break\n",
    "                \n",
    "                generated_tokens.append(next_token.item())\n",
    "                \n",
    "                # Prepare next iteration\n",
    "                next_embedding = self.model.decoder.token_embedding(next_token).unsqueeze(1)\n",
    "                decoder_input = torch.cat([decoder_input, next_embedding], dim=1)\n",
    "            \n",
    "            return self.model.tokenizer.decode(torch.tensor(generated_tokens))\n",
    "\n",
    "def quick_test():\n",
    "    print(\"Starting quick Whisper model test...\")\n",
    "    \n",
    "    print(\"Loading minimal test dataset...\")\n",
    "    dataset = load_dataset(\n",
    "        \"patrickvonplaten/librispeech_asr_dummy\",\n",
    "        \"clean\",\n",
    "        split=\"validation[:5]\"\n",
    "    )\n",
    "    \n",
    "    checkpoint_path = \"/private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_4_batch_0.pt\"\n",
    "    tester = WhisperTester(checkpoint_path)\n",
    "    \n",
    "    print(\"\\nTesting transcription on samples:\")\n",
    "    for idx, sample in enumerate(dataset):\n",
    "        print(f\"\\nSample {idx + 1}/5:\")\n",
    "        try:\n",
    "            audio_features = tester.process_audio(sample['audio']['array'])\n",
    "            transcription = tester.transcribe(audio_features)\n",
    "            \n",
    "            print(f\"Original: {sample['text']}\")\n",
    "            print(f\"Predicted: {transcription}\")\n",
    "            \n",
    "            original_words = set(sample['text'].lower().split())\n",
    "            predicted_words = set(transcription.lower().split())\n",
    "            common_words = original_words.intersection(predicted_words)\n",
    "            wer = 1 - (len(common_words) / len(original_words))\n",
    "            print(f\"Word Error Rate: {wer:.2%}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nQuick test completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    quick_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting quick Whisper model test...\n",
      "Testing model features...\n",
      "Using device: cpu\n",
      "Loading model from /private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_4_batch_0.pt\n",
      "Loaded model from epoch 4, loss: 0.0000\n",
      "\n",
      "Loading small test dataset...\n",
      "\n",
      "1. Testing Basic Transcription\n",
      "\n",
      "Sample 1/10:\n",
      "Original: A MAN SAID TO THE UNIVERSE SIR I EXIST\n",
      "Predicted: \n",
      "WER: 100.00%\n",
      "\n",
      "2. Testing Model Architecture\n",
      "Encoder output shape: torch.Size([1, 233, 512])\n",
      "Task embedding shape: torch.Size([512])\n",
      "\n",
      "3. Testing Attention Patterns\n",
      "Decoder block output shape: torch.Size([1, 1, 512])\n",
      "\n",
      "4. Testing Different Tasks\n",
      "\n",
      "Testing transcribe task:\n",
      "Task transcribe output shape: torch.Size([1, 1, 512])\n",
      "\n",
      "Testing translate task:\n",
      "Task translate output shape: torch.Size([1, 1, 512])\n",
      "\n",
      "Testing language_id task:\n",
      "Task language_id output shape: torch.Size([1, 1, 512])\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 2/10:\n",
      "Original: SWEAT COVERED BRION'S BODY TRICKLING INTO THE TIGHT LOINCLOTH THAT WAS THE ONLY GARMENT HE WORE\n",
      "Predicted: \n",
      "WER: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 3/10:\n",
      "Original: THE CUT ON HIS CHEST STILL DRIPPING BLOOD THE ACHE OF HIS OVERSTRAINED EYES EVEN THE SOARING ARENA AROUND HIM WITH THE THOUSANDS OF SPECTATORS WERE TRIVIALITIES NOT WORTH THINKING ABOUT\n",
      "Predicted: \n",
      "WER: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 4/10:\n",
      "Original: HIS INSTANT OF PANIC WAS FOLLOWED BY A SMALL SHARP BLOW HIGH ON HIS CHEST\n",
      "Predicted: \n",
      "WER: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 5/10:\n",
      "Original: ONE MINUTE A VOICE SAID AND THE TIME BUZZER SOUNDED\n",
      "Predicted: \n",
      "WER: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 6/10:\n",
      "Original: A MINUTE IS NOT A VERY LARGE MEASURE OF TIME AND HIS BODY NEEDED EVERY FRACTION OF IT\n",
      "Predicted: \n",
      "WER: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 7/10:\n",
      "Original: THE BUZZER'S WHIRR TRIGGERED HIS MUSCLES INTO COMPLETE RELAXATION\n",
      "Predicted: \n",
      "WER: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 8/10:\n",
      "Original: ONLY HIS HEART AND LUNGS WORKED ON AT A STRONG MEASURED RATE\n",
      "Predicted: \n",
      "WER: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 9/10:\n",
      "Original: HE WAS IN REVERIE SLIDING ALONG THE BORDERS OF CONSCIOUSNESS\n",
      "Predicted: \n",
      "WER: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 10/10:\n",
      "Original: THE CONTESTANTS IN THE TWENTIES NEEDED UNDISTURBED REST THEREFORE NIGHTS IN THE DORMITORIES WERE AS QUIET AS DEATH\n",
      "Predicted: \n",
      "WER: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "Feature testing completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_model_features():\n",
    "    \"\"\"Quick test of different model features on small dataset\"\"\"\n",
    "    print(\"Testing model features...\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint_path = \"/private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_4_batch_0.pt\"\n",
    "    tester = WhisperTester(checkpoint_path)\n",
    "    \n",
    "    # Load small test dataset\n",
    "    print(\"\\nLoading small test dataset...\")\n",
    "    dataset = load_dataset(\n",
    "        \"patrickvonplaten/librispeech_asr_dummy\",\n",
    "        \"clean\",\n",
    "        split=\"validation[:10]\"  # Use 10 samples\n",
    "    )\n",
    "    \n",
    "    print(\"\\n1. Testing Basic Transcription\")\n",
    "    for idx, sample in enumerate(dataset):\n",
    "        print(f\"\\nSample {idx + 1}/10:\")\n",
    "        try:\n",
    "            # Process audio\n",
    "            audio_features = tester.process_audio(sample['audio']['array'])\n",
    "            \n",
    "            # Test transcription\n",
    "            transcription = tester.transcribe(audio_features)\n",
    "            print(f\"Original: {sample['text']}\")\n",
    "            print(f\"Predicted: {transcription}\")\n",
    "            \n",
    "            # Simple WER\n",
    "            original_words = set(sample['text'].lower().split())\n",
    "            predicted_words = set(transcription.lower().split())\n",
    "            common_words = original_words.intersection(predicted_words)\n",
    "            wer = 1 - (len(common_words) / len(original_words))\n",
    "            print(f\"WER: {wer:.2%}\")\n",
    "            \n",
    "            # Test model internals on first sample only\n",
    "            if idx == 0:\n",
    "                print(\"\\n2. Testing Model Architecture\")\n",
    "                with torch.no_grad():\n",
    "                    # Check encoder output\n",
    "                    audio_features = audio_features.unsqueeze(0).to(tester.device)\n",
    "                    encoder_out = tester.model.encoder(audio_features)\n",
    "                    print(f\"Encoder output shape: {encoder_out.shape}\")\n",
    "                    \n",
    "                    # Check task token embeddings\n",
    "                    task_idx = tester.model.tokenizer.task_tokens['transcribe']\n",
    "                    task_emb = tester.model.task_tokens[task_idx - (tester.model.tokenizer.vocab_size - \n",
    "                                                                   len(tester.model.tokenizer.task_tokens))]\n",
    "                    print(f\"Task embedding shape: {task_emb.shape}\")\n",
    "                    \n",
    "                    # Check attention patterns\n",
    "                    print(\"\\n3. Testing Attention Patterns\")\n",
    "                    first_block = tester.model.decoder.blocks[0]\n",
    "                    decoder_input = tester.model.decoder.token_embedding(\n",
    "                        torch.tensor([[tester.model.tokenizer.bos_token]], device=tester.device)\n",
    "                    )\n",
    "                    decoder_out = first_block(decoder_input, encoder_out)\n",
    "                    print(f\"Decoder block output shape: {decoder_out.shape}\")\n",
    "                    \n",
    "                    # Test different tasks\n",
    "                    print(\"\\n4. Testing Different Tasks\")\n",
    "                    tasks = ['transcribe', 'translate', 'language_id']\n",
    "                    for task in tasks:\n",
    "                        print(f\"\\nTesting {task} task:\")\n",
    "                        try:\n",
    "                            task_token = torch.tensor([[\n",
    "                                tester.model.tokenizer.task_tokens[task]\n",
    "                            ]], device=tester.device)\n",
    "                            task_emb = tester.model.decoder.token_embedding(task_token)\n",
    "                            out = first_block(task_emb, encoder_out)\n",
    "                            print(f\"Task {task} output shape: {out.shape}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error testing {task}: {str(e)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {str(e)}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nFeature testing completed!\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Starting quick Whisper model test...\")\n",
    "        test_model_features()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Whisper multi-task test...\n",
      "Testing model features and multi-task capabilities...\n",
      "Using device: cpu\n",
      "Loading model from /private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_4_batch_0.pt\n",
      "Loaded model from epoch 4, loss: 0.0000\n",
      "\n",
      "Loading test dataset...\n",
      "\n",
      "Sample 1/10:\n",
      "Original text: A MAN SAID TO THE UNIVERSE SIR I EXIST\n",
      "\n",
      "Testing TRANSCRIBE task:\n",
      "Transcription: \n",
      "WER: 100.00%\n",
      "Output sequence length: 100\n",
      "Encoder attention shape: torch.Size([1, 233, 512])\n",
      "Decoder attention shape: torch.Size([1, 101, 512])\n",
      "\n",
      "Testing TRANSLATE task:\n",
      "Translation: \n",
      "Output sequence length: 100\n",
      "Encoder attention shape: torch.Size([1, 233, 512])\n",
      "Decoder attention shape: torch.Size([1, 101, 512])\n",
      "\n",
      "Testing LANGUAGE_ID task:\n",
      "Detected language: \n",
      "Output sequence length: 100\n",
      "Encoder attention shape: torch.Size([1, 233, 512])\n",
      "Decoder attention shape: torch.Size([1, 101, 512])\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 2/10:\n",
      "Original text: SWEAT COVERED BRION'S BODY TRICKLING INTO THE TIGHT LOINCLOTH THAT WAS THE ONLY GARMENT HE WORE\n",
      "\n",
      "Testing TRANSCRIBE task:\n",
      "Transcription: \n",
      "WER: 100.00%\n",
      "\n",
      "Testing TRANSLATE task:\n",
      "Translation: \n",
      "\n",
      "Testing LANGUAGE_ID task:\n",
      "Detected language: \n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 3/10:\n",
      "Original text: THE CUT ON HIS CHEST STILL DRIPPING BLOOD THE ACHE OF HIS OVERSTRAINED EYES EVEN THE SOARING ARENA AROUND HIM WITH THE THOUSANDS OF SPECTATORS WERE TRIVIALITIES NOT WORTH THINKING ABOUT\n",
      "\n",
      "Testing TRANSCRIBE task:\n",
      "Transcription: \n",
      "WER: 100.00%\n",
      "\n",
      "Testing TRANSLATE task:\n",
      "Translation: \n",
      "\n",
      "Testing LANGUAGE_ID task:\n",
      "Detected language: \n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 4/10:\n",
      "Original text: HIS INSTANT OF PANIC WAS FOLLOWED BY A SMALL SHARP BLOW HIGH ON HIS CHEST\n",
      "\n",
      "Testing TRANSCRIBE task:\n",
      "Transcription: \n",
      "WER: 100.00%\n",
      "\n",
      "Testing TRANSLATE task:\n",
      "Translation: \n",
      "\n",
      "Testing LANGUAGE_ID task:\n",
      "Detected language: \n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 5/10:\n",
      "Original text: ONE MINUTE A VOICE SAID AND THE TIME BUZZER SOUNDED\n",
      "\n",
      "Testing TRANSCRIBE task:\n",
      "Transcription: \n",
      "WER: 100.00%\n",
      "\n",
      "Testing TRANSLATE task:\n",
      "Translation: \n",
      "\n",
      "Testing LANGUAGE_ID task:\n",
      "Detected language: \n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 6/10:\n",
      "Original text: A MINUTE IS NOT A VERY LARGE MEASURE OF TIME AND HIS BODY NEEDED EVERY FRACTION OF IT\n",
      "\n",
      "Testing TRANSCRIBE task:\n",
      "Transcription: \n",
      "WER: 100.00%\n",
      "\n",
      "Testing TRANSLATE task:\n",
      "Translation: \n",
      "\n",
      "Testing LANGUAGE_ID task:\n",
      "Detected language: \n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 7/10:\n",
      "Original text: THE BUZZER'S WHIRR TRIGGERED HIS MUSCLES INTO COMPLETE RELAXATION\n",
      "\n",
      "Testing TRANSCRIBE task:\n",
      "Transcription: \n",
      "WER: 100.00%\n",
      "\n",
      "Testing TRANSLATE task:\n",
      "Translation: \n",
      "\n",
      "Testing LANGUAGE_ID task:\n",
      "Detected language: \n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 8/10:\n",
      "Original text: ONLY HIS HEART AND LUNGS WORKED ON AT A STRONG MEASURED RATE\n",
      "\n",
      "Testing TRANSCRIBE task:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 124\u001b[0m\n\u001b[1;32m    121\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 124\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 117\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Whisper multi-task test...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m     \u001b[43mtest_multitask_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during testing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 66\u001b[0m, in \u001b[0;36mtest_multitask_features\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     x \u001b[38;5;241m=\u001b[39m block(x, encoder_out)\n\u001b[1;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m tester\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[0;32m---> 66\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m next_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_token\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m tester\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token:\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/fsl/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def test_multitask_features():\n",
    "    \"\"\"Test different tasks and features of the model\"\"\"\n",
    "    print(\"Testing model features and multi-task capabilities...\")\n",
    "    \n",
    "    checkpoint_path = \"/private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_4_batch_0.pt\"\n",
    "    tester = WhisperTester(checkpoint_path)\n",
    "    \n",
    "    # Load test dataset\n",
    "    print(\"\\nLoading test dataset...\")\n",
    "    dataset = load_dataset(\n",
    "        \"patrickvonplaten/librispeech_asr_dummy\",\n",
    "        \"clean\",\n",
    "        split=\"validation[:10]\"\n",
    "    )\n",
    "    \n",
    "    # Test different tasks on each sample\n",
    "    tasks = ['transcribe', 'translate', 'language_id']\n",
    "    \n",
    "    for idx, sample in enumerate(dataset):\n",
    "        print(f\"\\nSample {idx + 1}/10:\")\n",
    "        print(f\"Original text: {sample['text']}\")\n",
    "        \n",
    "        try:\n",
    "            # Process audio\n",
    "            audio_features = tester.process_audio(sample['audio']['array'])\n",
    "            \n",
    "            # Test each task\n",
    "            for task in tasks:\n",
    "                print(f\"\\nTesting {task.upper()} task:\")\n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        # Prepare input\n",
    "                        audio_features_batch = audio_features.unsqueeze(0).to(tester.device)\n",
    "                        encoder_out = tester.model.encoder(audio_features_batch)\n",
    "                        \n",
    "                        # Get task token embedding\n",
    "                        task_idx = tester.model.tokenizer.task_tokens[task]\n",
    "                        task_embedding = tester.model.task_tokens[task_idx - (tester.model.tokenizer.vocab_size - \n",
    "                                                                           len(tester.model.tokenizer.task_tokens))]\n",
    "                        \n",
    "                        # Initialize decoder with task token\n",
    "                        batch_size = 1\n",
    "                        task_emb = task_embedding.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, -1)\n",
    "                        \n",
    "                        # Initialize decoder sequence\n",
    "                        decoder_input = tester.model.decoder.token_embedding(\n",
    "                            torch.tensor([[tester.model.tokenizer.bos_token]], device=tester.device)\n",
    "                        )\n",
    "                        decoder_input = torch.cat([task_emb, decoder_input], dim=1)\n",
    "                        \n",
    "                        # Generate sequence\n",
    "                        generated_tokens = []\n",
    "                        max_length = 100\n",
    "                        \n",
    "                        for _ in range(max_length):\n",
    "                            # Add positional embeddings\n",
    "                            pos_emb = tester.model.decoder.positional_embedding[:decoder_input.size(1), :].unsqueeze(0)\n",
    "                            current_input = decoder_input + pos_emb\n",
    "                            \n",
    "                            # Run through decoder\n",
    "                            x = current_input\n",
    "                            for block in tester.model.decoder.blocks:\n",
    "                                x = block(x, encoder_out)\n",
    "                            \n",
    "                            x = tester.model.decoder.ln(x)\n",
    "                            logits = tester.model.decoder.fc(x)\n",
    "                            next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "                            \n",
    "                            if next_token.item() == tester.model.tokenizer.eos_token:\n",
    "                                break\n",
    "                            \n",
    "                            generated_tokens.append(next_token.item())\n",
    "                            next_embedding = tester.model.decoder.token_embedding(next_token).unsqueeze(1)\n",
    "                            decoder_input = torch.cat([decoder_input, next_embedding], dim=1)\n",
    "                        \n",
    "                        # Decode output based on task\n",
    "                        output = tester.model.tokenizer.decode(torch.tensor(generated_tokens))\n",
    "                        \n",
    "                        if task == 'transcribe':\n",
    "                            print(f\"Transcription: {output}\")\n",
    "                            # Calculate WER\n",
    "                            original_words = set(sample['text'].lower().split())\n",
    "                            predicted_words = set(output.lower().split())\n",
    "                            common_words = original_words.intersection(predicted_words)\n",
    "                            wer = 1 - (len(common_words) / len(original_words))\n",
    "                            print(f\"WER: {wer:.2%}\")\n",
    "                            \n",
    "                        elif task == 'translate':\n",
    "                            print(f\"Translation: {output}\")\n",
    "                            \n",
    "                        elif task == 'language_id':\n",
    "                            print(f\"Detected language: {output}\")\n",
    "                            \n",
    "                        # Show attention visualization for first sample\n",
    "                        if idx == 0:\n",
    "                            print(f\"Output sequence length: {len(generated_tokens)}\")\n",
    "                            print(f\"Encoder attention shape: {encoder_out.shape}\")\n",
    "                            print(f\"Decoder attention shape: {x.shape}\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in {task} task: {str(e)}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nMulti-task testing completed!\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Starting Whisper multi-task test...\")\n",
    "        test_multitask_features()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model debugging...\n",
      "Using device: cpu\n",
      "Loading model from /private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_4_batch_0.pt\n",
      "Loaded model from epoch 4, loss: 0.0000\n",
      "\n",
      "Original text: A MAN SAID TO THE UNIVERSE SIR I EXIST\n",
      "\n",
      "Debugging transcribe task:\n",
      "Audio features shape: torch.Size([1, 80, 466])\n",
      "Audio features range: [-13.49, 8.98]\n",
      "Encoder output shape: torch.Size([1, 233, 512])\n",
      "Encoder output range: [-3.29, 4.09]\n",
      "Task token index: 51860\n",
      "Task embedding shape: torch.Size([512])\n",
      "Task embedding range: [-2.69, 2.90]\n",
      "Initial decoder input: tensor([[51860,     2]])\n",
      "Decoder embedding shape: torch.Size([1, 2, 512])\n",
      "Decoder embedding range: [-3.29, 3.10]\n",
      "Decoder logits shape: torch.Size([1, 2, 51865])\n",
      "Logits range: [-107.76, 330.92]\n",
      "\n",
      "Top 5 predictions:\n",
      "Token 2 (prob: 1.0000)\n",
      "Token 28525 (prob: 1.0000)\n",
      "Token 25907 (prob: 1.0000)\n",
      "Token 20667 (prob: 1.0000)\n",
      "Token 50180 (prob: 1.0000)\n",
      "\n",
      "Attempting transcribe generation:\n",
      "Error in generation: Tensors must have same number of dimensions: got 2 and 3\n",
      "--------------------------------------------------\n",
      "\n",
      "Debugging translate task:\n",
      "Audio features shape: torch.Size([1, 80, 466])\n",
      "Audio features range: [-13.49, 8.98]\n",
      "Encoder output shape: torch.Size([1, 233, 512])\n",
      "Encoder output range: [-3.29, 4.09]\n",
      "Task token index: 51861\n",
      "Task embedding shape: torch.Size([512])\n",
      "Task embedding range: [-2.86, 3.12]\n",
      "Initial decoder input: tensor([[51861,     2]])\n",
      "Decoder embedding shape: torch.Size([1, 2, 512])\n",
      "Decoder embedding range: [-3.29, 2.89]\n",
      "Decoder logits shape: torch.Size([1, 2, 51865])\n",
      "Logits range: [-157.42, 333.84]\n",
      "\n",
      "Top 5 predictions:\n",
      "Token 2 (prob: 1.0000)\n",
      "Token 21385 (prob: 1.0000)\n",
      "Token 25907 (prob: 1.0000)\n",
      "Token 48701 (prob: 1.0000)\n",
      "Token 50180 (prob: 1.0000)\n",
      "\n",
      "Attempting translate generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/ipykernel_49321/2688510538.py\", line 95, in test_model\n",
      "    current_input = torch.cat([\n",
      "                    ^^^^^^^^^^^\n",
      "RuntimeError: Tensors must have same number of dimensions: got 2 and 3\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/ipykernel_49321/2688510538.py\", line 95, in test_model\n",
      "    current_input = torch.cat([\n",
      "                    ^^^^^^^^^^^\n",
      "RuntimeError: Tensors must have same number of dimensions: got 2 and 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in generation: Tensors must have same number of dimensions: got 2 and 3\n",
      "--------------------------------------------------\n",
      "\n",
      "Debugging language_id task:\n",
      "Audio features shape: torch.Size([1, 80, 466])\n",
      "Audio features range: [-13.49, 8.98]\n",
      "Encoder output shape: torch.Size([1, 233, 512])\n",
      "Encoder output range: [-3.29, 4.09]\n",
      "Task token index: 51862\n",
      "Task embedding shape: torch.Size([512])\n",
      "Task embedding range: [-2.77, 3.63]\n",
      "Initial decoder input: tensor([[51862,     2]])\n",
      "Decoder embedding shape: torch.Size([1, 2, 512])\n",
      "Decoder embedding range: [-3.08, 3.51]\n",
      "Decoder logits shape: torch.Size([1, 2, 51865])\n",
      "Logits range: [-99.45, 325.62]\n",
      "\n",
      "Top 5 predictions:\n",
      "Token 2 (prob: 1.0000)\n",
      "Token 21385 (prob: 1.0000)\n",
      "Token 28525 (prob: 1.0000)\n",
      "Token 50180 (prob: 1.0000)\n",
      "Token 1565 (prob: 1.0000)\n",
      "\n",
      "Attempting language_id generation:\n",
      "Error in generation: Tensors must have same number of dimensions: got 2 and 3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/ipykernel_49321/2688510538.py\", line 95, in test_model\n",
      "    current_input = torch.cat([\n",
      "                    ^^^^^^^^^^^\n",
      "RuntimeError: Tensors must have same number of dimensions: got 2 and 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def debug_model_output(tester, audio_features, task='transcribe'):\n",
    "    \"\"\"Debug function to inspect model's internal states\"\"\"\n",
    "    print(f\"\\nDebugging {task} task:\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 1. Check audio features\n",
    "        audio_features = audio_features.unsqueeze(0).to(tester.device)\n",
    "        print(f\"Audio features shape: {audio_features.shape}\")\n",
    "        print(f\"Audio features range: [{audio_features.min():.2f}, {audio_features.max():.2f}]\")\n",
    "        \n",
    "        # 2. Check encoder output\n",
    "        encoder_out = tester.model.encoder(audio_features)\n",
    "        print(f\"Encoder output shape: {encoder_out.shape}\")\n",
    "        print(f\"Encoder output range: [{encoder_out.min():.2f}, {encoder_out.max():.2f}]\")\n",
    "        \n",
    "        # 3. Check task token\n",
    "        task_idx = tester.model.tokenizer.task_tokens[task]\n",
    "        print(f\"Task token index: {task_idx}\")\n",
    "        \n",
    "        # 4. Check task embedding\n",
    "        task_embedding = tester.model.task_tokens[task_idx - (tester.model.tokenizer.vocab_size - \n",
    "                                                           len(tester.model.tokenizer.task_tokens))]\n",
    "        print(f\"Task embedding shape: {task_embedding.shape}\")\n",
    "        print(f\"Task embedding range: [{task_embedding.min():.2f}, {task_embedding.max():.2f}]\")\n",
    "        \n",
    "        # 5. Check decoder input\n",
    "        decoder_input = torch.tensor([[\n",
    "            task_idx,\n",
    "            tester.model.tokenizer.bos_token\n",
    "        ]], device=tester.device)\n",
    "        print(f\"Initial decoder input: {decoder_input}\")\n",
    "        \n",
    "        # 6. Check decoder embedding\n",
    "        decoder_emb = tester.model.decoder.token_embedding(decoder_input)\n",
    "        print(f\"Decoder embedding shape: {decoder_emb.shape}\")\n",
    "        print(f\"Decoder embedding range: [{decoder_emb.min():.2f}, {decoder_emb.max():.2f}]\")\n",
    "        \n",
    "        # 7. Check decoder output\n",
    "        logits = tester.model.decoder(decoder_input, encoder_out)\n",
    "        print(f\"Decoder logits shape: {logits.shape}\")\n",
    "        print(f\"Logits range: [{logits.min():.2f}, {logits.max():.2f}]\")\n",
    "        \n",
    "        # 8. Check top predictions\n",
    "        top_probs, top_tokens = torch.topk(logits[0, -1], k=5)\n",
    "        print(\"\\nTop 5 predictions:\")\n",
    "        for prob, token in zip(top_probs, top_tokens):\n",
    "            if token.item() in tester.model.tokenizer.idx_to_char:\n",
    "                char = tester.model.tokenizer.idx_to_char[token.item()]\n",
    "                print(f\"Token {token.item()}: '{char}' (prob: {torch.softmax(prob, dim=0):.4f})\")\n",
    "            else:\n",
    "                print(f\"Token {token.item()} (prob: {torch.softmax(prob, dim=0):.4f})\")\n",
    "        \n",
    "        return encoder_out, logits\n",
    "\n",
    "def test_model():\n",
    "    print(\"Starting model debugging...\")\n",
    "    \n",
    "    checkpoint_path = \"/private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_4_batch_0.pt\"\n",
    "    tester = WhisperTester(checkpoint_path)\n",
    "    \n",
    "    # Load a single sample for debugging\n",
    "    dataset = load_dataset(\n",
    "        \"patrickvonplaten/librispeech_asr_dummy\",\n",
    "        \"clean\",\n",
    "        split=\"validation[:1]\"\n",
    "    )\n",
    "    \n",
    "    sample = dataset[0]\n",
    "    print(f\"\\nOriginal text: {sample['text']}\")\n",
    "    \n",
    "    # Process audio\n",
    "    audio_features = tester.process_audio(sample['audio']['array'])\n",
    "    \n",
    "    # Debug each task\n",
    "    for task in ['transcribe', 'translate', 'language_id']:\n",
    "        encoder_out, logits = debug_model_output(tester, audio_features, task)\n",
    "        \n",
    "        print(f\"\\nAttempting {task} generation:\")\n",
    "        try:\n",
    "            # Try to generate a few tokens\n",
    "            current_input = torch.tensor([[\n",
    "                tester.model.tokenizer.task_tokens[task],\n",
    "                tester.model.tokenizer.bos_token\n",
    "            ]], device=tester.device)\n",
    "            \n",
    "            generated_tokens = []\n",
    "            for _ in range(10):  # Try to generate first 10 tokens\n",
    "                logits = tester.model.decoder(current_input, encoder_out)\n",
    "                next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "                \n",
    "                if next_token.item() == tester.model.tokenizer.eos_token:\n",
    "                    break\n",
    "                    \n",
    "                generated_tokens.append(next_token.item())\n",
    "                current_input = torch.cat([\n",
    "                    current_input,\n",
    "                    next_token.unsqueeze(0).unsqueeze(0)\n",
    "                ], dim=1)\n",
    "                \n",
    "                # Print each generated token\n",
    "                if next_token.item() in tester.model.tokenizer.idx_to_char:\n",
    "                    print(f\"Generated token: {next_token.item()} -> '{tester.model.tokenizer.idx_to_char[next_token.item()]}'\")\n",
    "                else:\n",
    "                    print(f\"Generated token: {next_token.item()} (special token)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in generation: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting detailed model debugging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/ipykernel_49321/1250371534.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint contents:\n",
      "epoch: 0\n",
      "batch_idx: 0\n",
      "model_state_dict: dictionary with 265 items\n",
      "optimizer_state_dict: dictionary with 2 items\n",
      "loss: 0.651986837387085\n",
      "Using device: cpu\n",
      "Loading model from /private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_0_batch_0.pt\n",
      "Loaded model from epoch 0, loss: 0.6520\n",
      "\n",
      "Test sample text: A MAN SAID TO THE UNIVERSE SIR I EXIST\n",
      "\n",
      "Debugging transcribe task:\n",
      "\n",
      "Checking model weights:\n",
      "Encoder weights sum: 6553.4707\n",
      "Decoder weights sum: 8322.0371\n",
      "\n",
      "Audio features:\n",
      "Shape: torch.Size([1, 80, 466])\n",
      "Range: [-13.4864, 8.9791]\n",
      "Mean: -4.2057\n",
      "\n",
      "Encoder output:\n",
      "Shape: torch.Size([1, 233, 512])\n",
      "Range: [-3.6061, 4.4856]\n",
      "Mean: -0.0000\n",
      "\n",
      "Tokenizer check:\n",
      "Vocabulary size: 43\n",
      "Task token index: 51860\n",
      "\n",
      "Decoder embeddings:\n",
      "Shape: torch.Size([1, 2, 512])\n",
      "Range: [-3.2855, 3.1024]\n",
      "Mean: -0.0240\n",
      "\n",
      "Decoder block 0 attention:\n",
      "Query weights range: [-0.0443, 0.0443]\n",
      "\n",
      "Decoder block 1 attention:\n",
      "Query weights range: [-0.0443, 0.0443]\n",
      "\n",
      "Decoder block 2 attention:\n",
      "Query weights range: [-0.0443, 0.0443]\n",
      "\n",
      "Decoder block 3 attention:\n",
      "Query weights range: [-0.0443, 0.0443]\n",
      "\n",
      "Decoder block 4 attention:\n",
      "Query weights range: [-0.0443, 0.0443]\n",
      "\n",
      "Decoder block 5 attention:\n",
      "Query weights range: [-0.0443, 0.0443]\n",
      "\n",
      "Final logits:\n",
      "Shape: torch.Size([1, 2, 51865])\n",
      "Range: [-93.1292, 427.4754]\n",
      "Mean: -0.1750\n",
      "\n",
      "Top 5 token probabilities:\n",
      "Token 2: 1.0000\n",
      "Token 4 ('.'): 0.0000\n",
      "Token 0: 0.0000\n",
      "Token 1: 0.0000\n",
      "Token 3 (' '): 0.0000\n",
      "\n",
      "Testing token generation for transcribe:\n",
      "\n",
      "Token generation sequence:\n",
      "\n",
      "Step 1:\n",
      "  Token 2: 1.0000\n",
      "  Token 0: 0.0000\n",
      "  Token 1: 0.0000\n",
      "\n",
      "Step 2:\n",
      "  Token 2: 1.0000\n",
      "  Token 0: 0.0000\n",
      "  Token 1: 0.0000\n",
      "\n",
      "Step 3:\n",
      "  Token 2: 1.0000\n",
      "  Token 0: 0.0000\n",
      "  Token 1: 0.0000\n",
      "\n",
      "Step 4:\n",
      "  Token 2: 1.0000\n",
      "  Token 0: 0.0000\n",
      "  Token 1: 0.0000\n",
      "\n",
      "Step 5:\n",
      "  Token 2: 1.0000\n",
      "  Token 0: 0.0000\n",
      "  Token 1: 0.0000\n",
      "\n",
      "Step 6:\n",
      "  Token 2: 1.0000\n",
      "  Token 0: 0.0000\n",
      "  Token 1: 0.0000\n",
      "\n",
      "Step 7:\n",
      "  Token 2: 1.0000\n",
      "  Token 0: 0.0000\n",
      "  Token 1: 0.0000\n",
      "\n",
      "Step 8:\n",
      "  Token 2: 1.0000\n",
      "  Token 0: 0.0000\n",
      "  Token 1: 0.0000\n",
      "\n",
      "Step 9:\n",
      "  Token 2: 1.0000\n",
      "  Token 0: 0.0000\n",
      "  Token 1: 0.0000\n",
      "\n",
      "Step 10:\n",
      "  Token 2: 1.0000\n",
      "  Token 0: 0.0000\n",
      "  Token 1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def debug_model_output(tester, audio_features, task='transcribe'):\n",
    "    \"\"\"Debug function with weight verification\"\"\"\n",
    "    print(f\"\\nDebugging {task} task:\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Verify model weights\n",
    "        print(\"\\nChecking model weights:\")\n",
    "        encoder_weights = sum(p.sum() for p in tester.model.encoder.parameters())\n",
    "        decoder_weights = sum(p.sum() for p in tester.model.decoder.parameters())\n",
    "        print(f\"Encoder weights sum: {encoder_weights:.4f}\")\n",
    "        print(f\"Decoder weights sum: {decoder_weights:.4f}\")\n",
    "        \n",
    "        # 1. Check audio features\n",
    "        audio_features = audio_features.unsqueeze(0).to(tester.device)\n",
    "        print(f\"\\nAudio features:\")\n",
    "        print(f\"Shape: {audio_features.shape}\")\n",
    "        print(f\"Range: [{audio_features.min():.4f}, {audio_features.max():.4f}]\")\n",
    "        print(f\"Mean: {audio_features.mean():.4f}\")\n",
    "        \n",
    "        # 2. Check encoder\n",
    "        encoder_out = tester.model.encoder(audio_features)\n",
    "        print(f\"\\nEncoder output:\")\n",
    "        print(f\"Shape: {encoder_out.shape}\")\n",
    "        print(f\"Range: [{encoder_out.min():.4f}, {encoder_out.max():.4f}]\")\n",
    "        print(f\"Mean: {encoder_out.mean():.4f}\")\n",
    "        \n",
    "        # 3. Verify tokenizer and embeddings\n",
    "        print(f\"\\nTokenizer check:\")\n",
    "        vocab_size = len(tester.model.tokenizer.char_to_idx) + 10  # Special tokens\n",
    "        print(f\"Vocabulary size: {vocab_size}\")\n",
    "        print(f\"Task token index: {tester.model.tokenizer.task_tokens[task]}\")\n",
    "        \n",
    "        # 4. Check decoder embeddings\n",
    "        decoder_input = torch.tensor([[\n",
    "            tester.model.tokenizer.task_tokens[task],\n",
    "            tester.model.tokenizer.bos_token\n",
    "        ]], device=tester.device)\n",
    "        \n",
    "        print(f\"\\nDecoder embeddings:\")\n",
    "        emb = tester.model.decoder.token_embedding(decoder_input)\n",
    "        print(f\"Shape: {emb.shape}\")\n",
    "        print(f\"Range: [{emb.min():.4f}, {emb.max():.4f}]\")\n",
    "        print(f\"Mean: {emb.mean():.4f}\")\n",
    "        \n",
    "        # 5. Check attention weights\n",
    "        for i, block in enumerate(tester.model.decoder.blocks):\n",
    "            print(f\"\\nDecoder block {i} attention:\")\n",
    "            if hasattr(block.attn, 'query'):\n",
    "                q_weights = block.attn.query.weight\n",
    "                print(f\"Query weights range: [{q_weights.min():.4f}, {q_weights.max():.4f}]\")\n",
    "        \n",
    "        # 6. Final output check\n",
    "        logits = tester.model.decoder(decoder_input, encoder_out)\n",
    "        print(f\"\\nFinal logits:\")\n",
    "        print(f\"Shape: {logits.shape}\")\n",
    "        print(f\"Range: [{logits.min():.4f}, {logits.max():.4f}]\")\n",
    "        print(f\"Mean: {logits.mean():.4f}\")\n",
    "        \n",
    "        # 7. Token distribution\n",
    "        probs = F.softmax(logits[0, -1], dim=-1)\n",
    "        top_probs, top_tokens = torch.topk(probs, k=5)\n",
    "        print(\"\\nTop 5 token probabilities:\")\n",
    "        for prob, token in zip(top_probs, top_tokens):\n",
    "            if token.item() in tester.model.tokenizer.idx_to_char:\n",
    "                char = tester.model.tokenizer.idx_to_char[token.item()]\n",
    "                print(f\"Token {token.item()} ('{char}'): {prob:.4f}\")\n",
    "            else:\n",
    "                print(f\"Token {token.item()}: {prob:.4f}\")\n",
    "        \n",
    "        return encoder_out, logits\n",
    "\n",
    "def test_model():\n",
    "    print(\"Starting detailed model debugging...\")\n",
    "    \n",
    "    checkpoint_path = \"/private/var/folders/nt/mw9vwj4s4d341b0fpfl28ykr0000gn/T/whisper_checkpoints/whisper_best_epoch_0_batch_0.pt\"\n",
    "    \n",
    "    # Load checkpoint directly to inspect\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    print(\"\\nCheckpoint contents:\")\n",
    "    for key, value in checkpoint.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"{key}: tensor of shape {value.shape}\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"{key}: dictionary with {len(value)} items\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Initialize tester\n",
    "    tester = WhisperTester(checkpoint_path)\n",
    "    \n",
    "    # Load test sample\n",
    "    dataset = load_dataset(\n",
    "        \"patrickvonplaten/librispeech_asr_dummy\",\n",
    "        \"clean\",\n",
    "        split=\"validation[:1]\"\n",
    "    )\n",
    "    \n",
    "    sample = dataset[0]\n",
    "    print(f\"\\nTest sample text: {sample['text']}\")\n",
    "    \n",
    "    # Process and debug\n",
    "    audio_features = tester.process_audio(sample['audio']['array'])\n",
    "    \n",
    "    for task in ['transcribe']:  # Start with just transcription\n",
    "        encoder_out, logits = debug_model_output(tester, audio_features, task)\n",
    "        \n",
    "        print(f\"\\nTesting token generation for {task}:\")\n",
    "        current_input = torch.tensor([[\n",
    "            tester.model.tokenizer.task_tokens[task],\n",
    "            tester.model.tokenizer.bos_token\n",
    "        ]], device=tester.device)\n",
    "        \n",
    "        print(\"\\nToken generation sequence:\")\n",
    "        for i in range(10):\n",
    "            with torch.no_grad():\n",
    "                logits = tester.model.decoder(current_input, encoder_out)\n",
    "                probs = F.softmax(logits[0, -1], dim=-1)\n",
    "                \n",
    "                # Print probability distribution\n",
    "                print(f\"\\nStep {i+1}:\")\n",
    "                top_probs, top_tokens = torch.topk(probs, k=3)\n",
    "                for prob, token in zip(top_probs, top_tokens):\n",
    "                    if token.item() in tester.model.tokenizer.idx_to_char:\n",
    "                        char = tester.model.tokenizer.idx_to_char[token.item()]\n",
    "                        print(f\"  Token {token.item()} ('{char}'): {prob:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"  Token {token.item()}: {prob:.4f}\")\n",
    "                \n",
    "                next_token = torch.argmax(logits[0, -1])\n",
    "                if next_token.item() == tester.model.tokenizer.eos_token:\n",
    "                    print(\"  Generated EOS token, stopping.\")\n",
    "                    break\n",
    "                \n",
    "                current_input = torch.cat([\n",
    "                    current_input,\n",
    "                    next_token.unsqueeze(0).unsqueeze(0)\n",
    "                ], dim=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Whisper's multi-task capabilities...\n",
      "\n",
      "Test sample text: A MAN SAID TO THE UNIVERSE SIR I EXIST\n",
      "\n",
      "=== Testing TRANSCRIBE task ===\n",
      "Audio features shape: torch.Size([1, 80, 466])\n",
      "Encoder output shape: torch.Size([1, 233, 512])\n",
      "Task token index: 51860\n",
      "Task embedding shape: torch.Size([512])\n",
      "Initial decoder input shape: torch.Size([1, 2, 512])\n",
      "\n",
      "Generation process:\n",
      "Block 1 output shape: torch.Size([1, 2, 512])\n",
      "Block 2 output shape: torch.Size([1, 2, 512])\n",
      "Block 3 output shape: torch.Size([1, 2, 512])\n",
      "Block 4 output shape: torch.Size([1, 2, 512])\n",
      "Block 5 output shape: torch.Size([1, 2, 512])\n",
      "Block 6 output shape: torch.Size([1, 2, 512])\n",
      "\n",
      "Step 1:\n",
      "Logits shape: torch.Size([1, 2, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Step 2:\n",
      "Logits shape: torch.Size([1, 3, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Step 3:\n",
      "Logits shape: torch.Size([1, 4, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Step 4:\n",
      "Logits shape: torch.Size([1, 5, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Step 5:\n",
      "Logits shape: torch.Size([1, 6, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Task output: \n",
      "Calculating WER for transcription:\n",
      "WER: 100.00%\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Testing TRANSLATE task ===\n",
      "Audio features shape: torch.Size([1, 80, 466])\n",
      "Encoder output shape: torch.Size([1, 233, 512])\n",
      "Task token index: 51861\n",
      "Task embedding shape: torch.Size([512])\n",
      "Initial decoder input shape: torch.Size([1, 2, 512])\n",
      "\n",
      "Generation process:\n",
      "Block 1 output shape: torch.Size([1, 2, 512])\n",
      "Block 2 output shape: torch.Size([1, 2, 512])\n",
      "Block 3 output shape: torch.Size([1, 2, 512])\n",
      "Block 4 output shape: torch.Size([1, 2, 512])\n",
      "Block 5 output shape: torch.Size([1, 2, 512])\n",
      "Block 6 output shape: torch.Size([1, 2, 512])\n",
      "\n",
      "Step 1:\n",
      "Logits shape: torch.Size([1, 2, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Step 2:\n",
      "Logits shape: torch.Size([1, 3, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Step 3:\n",
      "Logits shape: torch.Size([1, 4, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Step 4:\n",
      "Logits shape: torch.Size([1, 5, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Step 5:\n",
      "Logits shape: torch.Size([1, 6, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Task output: \n",
      "--------------------------------------------------\n",
      "\n",
      "=== Testing LANGUAGE_ID task ===\n",
      "Audio features shape: torch.Size([1, 80, 466])\n",
      "Encoder output shape: torch.Size([1, 233, 512])\n",
      "Task token index: 51862\n",
      "Task embedding shape: torch.Size([512])\n",
      "Initial decoder input shape: torch.Size([1, 2, 512])\n",
      "\n",
      "Generation process:\n",
      "Block 1 output shape: torch.Size([1, 2, 512])\n",
      "Block 2 output shape: torch.Size([1, 2, 512])\n",
      "Block 3 output shape: torch.Size([1, 2, 512])\n",
      "Block 4 output shape: torch.Size([1, 2, 512])\n",
      "Block 5 output shape: torch.Size([1, 2, 512])\n",
      "Block 6 output shape: torch.Size([1, 2, 512])\n",
      "\n",
      "Step 1:\n",
      "Logits shape: torch.Size([1, 2, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Step 2:\n",
      "Logits shape: torch.Size([1, 3, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Step 3:\n",
      "Logits shape: torch.Size([1, 4, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Step 4:\n",
      "Logits shape: torch.Size([1, 5, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Step 5:\n",
      "Logits shape: torch.Size([1, 6, 51865])\n",
      "Selected token: 2\n",
      "\n",
      "Task output: \n",
      "--------------------------------------------------\n",
      "\n",
      "Multi-task demonstration completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def process_audio(audio_array: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Process audio array into mel spectrogram\"\"\"\n",
    "    # Convert to tensor\n",
    "    waveform = torch.from_numpy(audio_array).float()\n",
    "    if len(waveform.shape) > 1:\n",
    "        waveform = waveform.mean(dim=0)\n",
    "        \n",
    "    # Setup mel spectrogram transform\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=16000,\n",
    "        n_mels=80,\n",
    "        n_fft=2048,\n",
    "        hop_length=160,\n",
    "        win_length=400\n",
    "    )\n",
    "    \n",
    "    # Compute mel spectrogram\n",
    "    mel_spec = mel_transform(waveform)\n",
    "    mel_spec = torch.log(mel_spec + 1e-9)\n",
    "    \n",
    "    return mel_spec\n",
    "\n",
    "def test_multitask_features():\n",
    "    \"\"\"Demonstrate how Whisper handles different tasks with the same architecture\"\"\"\n",
    "    print(\"Testing Whisper's multi-task capabilities...\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = WhisperModel(\n",
    "        n_mels=80,\n",
    "        n_vocab=51865,\n",
    "        n_state=512,\n",
    "        n_head=8,\n",
    "        n_layer=6\n",
    "    )\n",
    "    \n",
    "    # Load a single test sample\n",
    "    dataset = load_dataset(\n",
    "        \"patrickvonplaten/librispeech_asr_dummy\",\n",
    "        \"clean\",\n",
    "        split=\"validation[:1]\"\n",
    "    )\n",
    "    sample = dataset[0]\n",
    "    print(f\"\\nTest sample text: {sample['text']}\")\n",
    "    \n",
    "    # Process audio\n",
    "    audio_features = process_audio(sample['audio']['array'])\n",
    "    \n",
    "    # Test each task's processing pipeline\n",
    "    tasks = ['transcribe', 'translate', 'language_id']\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for task in tasks:\n",
    "        print(f\"\\n=== Testing {task.upper()} task ===\")\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # 1. Encode audio (shared for all tasks)\n",
    "                audio_features_batch = audio_features.unsqueeze(0).to(device)\n",
    "                print(f\"Audio features shape: {audio_features_batch.shape}\")\n",
    "                \n",
    "                encoder_out = model.encoder(audio_features_batch)\n",
    "                print(f\"Encoder output shape: {encoder_out.shape}\")\n",
    "                \n",
    "                # 2. Task-specific processing\n",
    "                task_idx = model.tokenizer.task_tokens[task]\n",
    "                task_embedding = model.task_tokens[task_idx - (model.tokenizer.vocab_size - \n",
    "                                                           len(model.tokenizer.task_tokens))]\n",
    "                print(f\"Task token index: {task_idx}\")\n",
    "                print(f\"Task embedding shape: {task_embedding.shape}\")\n",
    "                \n",
    "                # 3. Initialize decoder with task token\n",
    "                task_emb = task_embedding.unsqueeze(0).unsqueeze(0)\n",
    "                decoder_input = model.decoder.token_embedding(\n",
    "                    torch.tensor([[model.tokenizer.bos_token]], device=device)\n",
    "                )\n",
    "                decoder_input = torch.cat([task_emb, decoder_input], dim=1)\n",
    "                print(f\"Initial decoder input shape: {decoder_input.shape}\")\n",
    "                \n",
    "                # 4. Generate sequence with task-specific behavior\n",
    "                print(\"\\nGeneration process:\")\n",
    "                generated_tokens = []\n",
    "                for step in range(5):  # Show first 5 steps\n",
    "                    # Add positional embeddings\n",
    "                    pos_emb = model.decoder.positional_embedding[:decoder_input.size(1), :].unsqueeze(0)\n",
    "                    current_input = decoder_input + pos_emb\n",
    "                    \n",
    "                    # Run through decoder blocks\n",
    "                    x = current_input\n",
    "                    for block_idx, block in enumerate(model.decoder.blocks):\n",
    "                        x = block(x, encoder_out)\n",
    "                        if step == 0:  # Show shapes for first step\n",
    "                            print(f\"Block {block_idx + 1} output shape: {x.shape}\")\n",
    "                    \n",
    "                    # Get next token\n",
    "                    x = model.decoder.ln(x)\n",
    "                    logits = model.decoder.fc(x)\n",
    "                    next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "                    \n",
    "                    # Show token info\n",
    "                    print(f\"\\nStep {step + 1}:\")\n",
    "                    print(f\"Logits shape: {logits.shape}\")\n",
    "                    print(f\"Selected token: {next_token.item()}\")\n",
    "                    \n",
    "                    if next_token.item() == model.tokenizer.eos_token:\n",
    "                        print(\"Generated EOS token, stopping.\")\n",
    "                        break\n",
    "                    \n",
    "                    generated_tokens.append(next_token.item())\n",
    "                    next_embedding = model.decoder.token_embedding(next_token).unsqueeze(1)\n",
    "                    decoder_input = torch.cat([decoder_input, next_embedding], dim=1)\n",
    "                \n",
    "                # 5. Task-specific output processing\n",
    "                output = model.tokenizer.decode(torch.tensor(generated_tokens))\n",
    "                print(f\"\\nTask output: {output}\")\n",
    "                \n",
    "                if task == 'transcribe':\n",
    "                    print(\"Calculating WER for transcription:\")\n",
    "                    original_words = set(sample['text'].lower().split())\n",
    "                    predicted_words = set(output.lower().split())\n",
    "                    common_words = original_words.intersection(predicted_words)\n",
    "                    wer = 1 - (len(common_words) / len(original_words))\n",
    "                    print(f\"WER: {wer:.2%}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {task} task: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nMulti-task demonstration completed!\")\n",
    "\n",
    "def main():\n",
    "    test_multitask_features()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Whisper's multi-task capabilities using pretrained model...\n",
      "Loaded Whisper model: ModelDimensions(n_mels=80, n_audio_ctx=1500, n_audio_state=768, n_audio_head=12, n_audio_layer=12, n_vocab=51865, n_text_ctx=448, n_text_state=768, n_text_head=12, n_text_layer=12)\n",
      "\n",
      "Test sample text: A MAN SAID TO THE UNIVERSE SIR I EXIST\n",
      "\n",
      "=== TRANSCRIPTION TASK ===\n",
      "Transcription:  A man said to the universe, Sir, I exist.\n",
      "Language: en\n",
      "\n",
      "Segments:\n",
      "Segment 1:\n",
      "Text:  A man said to the universe, Sir, I exist.\n",
      "Timestamp: 0.00s - 4.24s\n",
      "\n",
      "=== TRANSLATION TASK ===\n",
      "Translation:  A man said to the universe, Sir, I exist.\n",
      "\n",
      "=== LANGUAGE DETECTION ===\n",
      "Detected language: en\n",
      "\n",
      "Language probabilities:\n",
      "en: 99.24%\n",
      "ar: 0.20%\n",
      "nn: 0.10%\n",
      "la: 0.09%\n",
      "cy: 0.09%\n",
      "\n",
      "=== MODEL INTERNALS ===\n",
      "\n",
      "1. Audio Processing:\n",
      "Input audio shape: 74400\n",
      "Mel spectrogram shape: torch.Size([80, 3000])\n",
      "Model dimensions: ModelDimensions(n_mels=80, n_audio_ctx=1500, n_audio_state=768, n_audio_head=12, n_audio_layer=12, n_vocab=51865, n_text_ctx=448, n_text_state=768, n_text_head=12, n_text_layer=12)\n",
      "\n",
      "2. Multi-task Processing:\n",
      "\n",
      "Processing with task: transcribe\n",
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n",
      "[00:00.000 --> 00:04.240]  A man said to the universe, Sir, I exist.\n",
      "\n",
      "Task output:\n",
      "Text:  A man said to the universe, Sir, I exist.\n",
      "Language: en\n",
      "\n",
      "Timing analysis:\n",
      "Start time: 0.00s\n",
      "End time: 4.24s\n",
      "Average log probability: -0.32\n",
      "Temperature used: 0.00\n",
      "Compression ratio: 0.84\n",
      "No speech probability: 0.01\n",
      "\n",
      "Processing with task: translate\n",
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n",
      "[00:00.000 --> 00:04.240]  A man said to the universe, Sir, I exist.\n",
      "\n",
      "Task output:\n",
      "Text:  A man said to the universe, Sir, I exist.\n",
      "Language: en\n",
      "\n",
      "Timing analysis:\n",
      "Start time: 0.00s\n",
      "End time: 4.24s\n",
      "Average log probability: -0.32\n",
      "Temperature used: 0.00\n",
      "Compression ratio: 0.84\n",
      "No speech probability: 0.01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import whisper\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "def test_whisper_multitask():\n",
    "    \"\"\"Demonstrate Whisper's multi-task capabilities using pretrained model\"\"\"\n",
    "    print(\"Testing Whisper's multi-task capabilities using pretrained model...\")\n",
    "    \n",
    "    # Load pretrained Whisper model\n",
    "    model = whisper.load_model(\"small\")\n",
    "    print(f\"Loaded Whisper model: {model.dims}\")\n",
    "    \n",
    "    # Load test sample\n",
    "    dataset = load_dataset(\n",
    "        \"patrickvonplaten/librispeech_asr_dummy\",\n",
    "        \"clean\",\n",
    "        split=\"validation[:1]\"\n",
    "    )\n",
    "    sample = dataset[0]\n",
    "    print(f\"\\nTest sample text: {sample['text']}\")\n",
    "    \n",
    "    # Convert audio to float32 numpy array\n",
    "    audio = sample['audio']['array'].astype(np.float32)\n",
    "    \n",
    "    # 1. Transcription\n",
    "    print(\"\\n=== TRANSCRIPTION TASK ===\")\n",
    "    transcribe_result = model.transcribe(audio)\n",
    "    print(f\"Transcription: {transcribe_result['text']}\")\n",
    "    print(f\"Language: {transcribe_result['language']}\")\n",
    "    \n",
    "    # Show segments\n",
    "    print(\"\\nSegments:\")\n",
    "    for idx, segment in enumerate(transcribe_result[\"segments\"]):\n",
    "        print(f\"Segment {idx + 1}:\")\n",
    "        print(f\"Text: {segment['text']}\")\n",
    "        print(f\"Timestamp: {segment['start']:.2f}s - {segment['end']:.2f}s\")\n",
    "    \n",
    "    # 2. Translation\n",
    "    print(\"\\n=== TRANSLATION TASK ===\")\n",
    "    translate_result = model.transcribe(audio, task=\"translate\")\n",
    "    print(f\"Translation: {translate_result['text']}\")\n",
    "    \n",
    "    # 3. Language Detection\n",
    "    print(\"\\n=== LANGUAGE DETECTION ===\")\n",
    "    audio_input = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(audio_input).to(model.device)\n",
    "    _, probs = model.detect_language(mel)\n",
    "    detected_lang = max(probs, key=probs.get)\n",
    "    print(f\"Detected language: {detected_lang}\")\n",
    "    print(\"\\nLanguage probabilities:\")\n",
    "    for lang, prob in sorted(probs.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"{lang}: {prob:.2%}\")\n",
    "    \n",
    "    # Show model internals\n",
    "    print(\"\\n=== MODEL INTERNALS ===\")\n",
    "    print(\"\\n1. Audio Processing:\")\n",
    "    print(f\"Input audio shape: {len(audio)}\")\n",
    "    print(f\"Mel spectrogram shape: {mel.shape}\")\n",
    "    print(f\"Model dimensions: {model.dims}\")\n",
    "    \n",
    "    # Demonstrate multi-task processing\n",
    "    print(\"\\n2. Multi-task Processing:\")\n",
    "    tasks = [\"transcribe\", \"translate\"]\n",
    "    \n",
    "    for task in tasks:\n",
    "        print(f\"\\nProcessing with task: {task}\")\n",
    "        result = model.transcribe(\n",
    "            audio,\n",
    "            task=task,\n",
    "            temperature=0,  # Use greedy decoding\n",
    "            verbose=True    # Show processing details\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTask output:\")\n",
    "        print(f\"Text: {result['text']}\")\n",
    "        print(f\"Language: {result['language']}\")\n",
    "        \n",
    "        # Show timing information\n",
    "        if len(result['segments']) > 0:\n",
    "            print(\"\\nTiming analysis:\")\n",
    "            segment = result['segments'][0]\n",
    "            print(f\"Start time: {segment['start']:.2f}s\")\n",
    "            print(f\"End time: {segment['end']:.2f}s\")\n",
    "            print(f\"Average log probability: {segment['avg_logprob']:.2f}\")\n",
    "            if 'temperature' in segment:\n",
    "                print(f\"Temperature used: {segment['temperature']:.2f}\")\n",
    "            if 'compression_ratio' in segment:\n",
    "                print(f\"Compression ratio: {segment['compression_ratio']:.2f}\")\n",
    "            if 'no_speech_prob' in segment:\n",
    "                print(f\"No speech probability: {segment['no_speech_prob']:.2f}\")\n",
    "\n",
    "def main():\n",
    "    test_whisper_multitask()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
